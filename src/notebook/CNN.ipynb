{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from https://github.com/vzhou842/cnn-from-scratch\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Note: In this implementation, we assume the input is a 2d numpy array for simplicity, because that's\n",
    "how our MNIST images are stored. This works for us because we use it as the first layer in our\n",
    "network, but most CNNs have many more Conv layers. If we were building a bigger network that needed\n",
    "to use Conv3x3 multiple times, we'd have to make the input be a 3d numpy array.\n",
    "'''\n",
    "\n",
    "class Conv3x3:\n",
    "  # A Convolution layer using 3x3 filters.\n",
    "\n",
    "  def __init__(self, num_filters):\n",
    "    self.num_filters = num_filters\n",
    "\n",
    "    # filters is a 3d array with dimensions (num_filters, 3, 3)\n",
    "    # We divide by 9 to reduce the variance of our initial values\n",
    "    self.filters = np.random.randn(num_filters, 3, 3) / 9\n",
    "\n",
    "  def iterate_regions(self, image):\n",
    "    '''\n",
    "    Generates all possible 3x3 image regions using valid padding.\n",
    "    - image is a 2d numpy array.\n",
    "    '''\n",
    "    h, w = image.shape\n",
    "\n",
    "    for i in range(h - 2):\n",
    "      for j in range(w - 2):\n",
    "        im_region = image[i:(i + 3), j:(j + 3)]\n",
    "        yield im_region, i, j\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the conv layer using the given input.\n",
    "    Returns a 3d numpy array with dimensions (h, w, num_filters).\n",
    "    - input is a 2d numpy array\n",
    "    '''\n",
    "    self.last_input = input\n",
    "\n",
    "    h, w = input.shape\n",
    "    output = np.zeros((h - 2, w - 2, self.num_filters))\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(input):\n",
    "      output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n",
    "\n",
    "    return output\n",
    "\n",
    "  def backprop(self, d_L_d_out, learn_rate):\n",
    "    '''\n",
    "    Performs a backward pass of the conv layer.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    - learn_rate is a float.\n",
    "    '''\n",
    "    d_L_d_filters = np.zeros(self.filters.shape)\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "      for f in range(self.num_filters):\n",
    "        d_L_d_filters[f] += d_L_d_out[i, j, f] * im_region\n",
    "\n",
    "    # Update filters\n",
    "    self.filters -= learn_rate * d_L_d_filters\n",
    "\n",
    "    # We aren't returning anything here since we use Conv3x3 as the first layer in our CNN.\n",
    "    # Otherwise, we'd need to return the loss gradient for this layer's inputs, just like every\n",
    "    # other layer in our CNN.\n",
    "    return None\n",
    "\n",
    "\n",
    "class MaxPool2:\n",
    "  # A Max Pooling layer using a pool size of 2.\n",
    "\n",
    "  def iterate_regions(self, image):\n",
    "    '''\n",
    "    Generates non-overlapping 2x2 image regions to pool over.\n",
    "    - image is a 2d numpy array\n",
    "    '''\n",
    "    h, w, _ = image.shape\n",
    "    new_h = h // 2\n",
    "    new_w = w // 2\n",
    "\n",
    "    for i in range(new_h):\n",
    "      for j in range(new_w):\n",
    "        im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n",
    "        yield im_region, i, j\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the maxpool layer using the given input.\n",
    "    Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n",
    "    - input is a 3d numpy array with dimensions (h, w, num_filters)\n",
    "    '''\n",
    "    self.last_input = input\n",
    "\n",
    "    h, w, num_filters = input.shape\n",
    "    output = np.zeros((h // 2, w // 2, num_filters))\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(input):\n",
    "      output[i, j] = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "    return output\n",
    "\n",
    "  def backprop(self, d_L_d_out):\n",
    "    '''\n",
    "    Performs a backward pass of the maxpool layer.\n",
    "    Returns the loss gradient for this layer's inputs.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    '''\n",
    "    d_L_d_input = np.zeros(self.last_input.shape)\n",
    "\n",
    "    for im_region, i, j in self.iterate_regions(self.last_input):\n",
    "      h, w, f = im_region.shape\n",
    "      amax = np.amax(im_region, axis=(0, 1))\n",
    "\n",
    "      for i2 in range(h):\n",
    "        for j2 in range(w):\n",
    "          for f2 in range(f):\n",
    "            # If this pixel was the max value, copy the gradient to it.\n",
    "            if im_region[i2, j2, f2] == amax[f2]:\n",
    "              d_L_d_input[i * 2 + i2, j * 2 + j2, f2] = d_L_d_out[i, j, f2]\n",
    "\n",
    "    return d_L_d_input\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "  # A standard fully-connected layer with softmax activation.\n",
    "\n",
    "  def __init__(self, input_len, nodes):\n",
    "    # We divide by input_len to reduce the variance of our initial values\n",
    "    self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "    self.biases = np.zeros(nodes)\n",
    "\n",
    "  def forward(self, input):\n",
    "    '''\n",
    "    Performs a forward pass of the softmax layer using the given input.\n",
    "    Returns a 1d numpy array containing the respective probability values.\n",
    "    - input can be any array with any dimensions.\n",
    "    '''\n",
    "    self.last_input_shape = input.shape\n",
    "\n",
    "    input = input.flatten()\n",
    "    self.last_input = input\n",
    "\n",
    "    input_len, nodes = self.weights.shape\n",
    "\n",
    "    totals = np.dot(input, self.weights) + self.biases\n",
    "    self.last_totals = totals\n",
    "\n",
    "    exp = np.exp(totals)\n",
    "    return exp / np.sum(exp, axis=0)\n",
    "\n",
    "  def backprop(self, d_L_d_out, learn_rate):\n",
    "    '''\n",
    "    Performs a backward pass of the softmax layer.\n",
    "    Returns the loss gradient for this layer's inputs.\n",
    "    - d_L_d_out is the loss gradient for this layer's outputs.\n",
    "    - learn_rate is a float.\n",
    "    '''\n",
    "    # We know only 1 element of d_L_d_out will be nonzero\n",
    "    for i, gradient in enumerate(d_L_d_out):\n",
    "      if gradient == 0:\n",
    "        continue\n",
    "\n",
    "      # e^totals\n",
    "      t_exp = np.exp(self.last_totals)\n",
    "\n",
    "      # Sum of all e^totals\n",
    "      S = np.sum(t_exp)\n",
    "\n",
    "      # Gradients of out[i] against totals\n",
    "      d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "      d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "\n",
    "      # Gradients of totals against weights/biases/input\n",
    "      d_t_d_w = self.last_input\n",
    "      d_t_d_b = 1\n",
    "      d_t_d_inputs = self.weights\n",
    "\n",
    "      # Gradients of loss against totals\n",
    "      d_L_d_t = gradient * d_out_d_t\n",
    "\n",
    "      # Gradients of loss against weights/biases/input\n",
    "      d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "      d_L_d_b = d_L_d_t * d_t_d_b\n",
    "      d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "\n",
    "      # Update weights / biases\n",
    "      self.weights -= learn_rate * d_L_d_w\n",
    "      self.biases -= learn_rate * d_L_d_b\n",
    "\n",
    "      return d_L_d_inputs.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST CNN initialized!\n",
      "--- Epoch 1 ---\n",
      "[Step 100] Past 100 steps: Average Loss 2.176 | Accuracy: 23%\n",
      "[Step 200] Past 100 steps: Average Loss 1.852 | Accuracy: 49%\n",
      "[Step 300] Past 100 steps: Average Loss 1.277 | Accuracy: 63%\n",
      "[Step 400] Past 100 steps: Average Loss 1.066 | Accuracy: 66%\n",
      "[Step 500] Past 100 steps: Average Loss 0.967 | Accuracy: 69%\n",
      "[Step 600] Past 100 steps: Average Loss 0.810 | Accuracy: 74%\n",
      "[Step 700] Past 100 steps: Average Loss 0.844 | Accuracy: 74%\n",
      "[Step 800] Past 100 steps: Average Loss 0.451 | Accuracy: 86%\n",
      "[Step 900] Past 100 steps: Average Loss 0.503 | Accuracy: 83%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.749 | Accuracy: 78%\n",
      "--- Epoch 2 ---\n",
      "[Step 100] Past 100 steps: Average Loss 0.539 | Accuracy: 85%\n",
      "[Step 200] Past 100 steps: Average Loss 0.662 | Accuracy: 80%\n",
      "[Step 300] Past 100 steps: Average Loss 0.427 | Accuracy: 91%\n",
      "[Step 400] Past 100 steps: Average Loss 0.525 | Accuracy: 83%\n",
      "[Step 500] Past 100 steps: Average Loss 0.398 | Accuracy: 88%\n",
      "[Step 600] Past 100 steps: Average Loss 0.448 | Accuracy: 89%\n",
      "[Step 700] Past 100 steps: Average Loss 0.376 | Accuracy: 91%\n",
      "[Step 800] Past 100 steps: Average Loss 0.402 | Accuracy: 90%\n",
      "[Step 900] Past 100 steps: Average Loss 0.467 | Accuracy: 89%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.725 | Accuracy: 77%\n",
      "--- Epoch 3 ---\n",
      "[Step 100] Past 100 steps: Average Loss 0.326 | Accuracy: 90%\n",
      "[Step 200] Past 100 steps: Average Loss 0.374 | Accuracy: 90%\n",
      "[Step 300] Past 100 steps: Average Loss 0.344 | Accuracy: 92%\n",
      "[Step 400] Past 100 steps: Average Loss 0.305 | Accuracy: 87%\n",
      "[Step 500] Past 100 steps: Average Loss 0.393 | Accuracy: 91%\n",
      "[Step 600] Past 100 steps: Average Loss 0.423 | Accuracy: 85%\n",
      "[Step 700] Past 100 steps: Average Loss 0.404 | Accuracy: 84%\n",
      "[Step 800] Past 100 steps: Average Loss 0.347 | Accuracy: 91%\n",
      "[Step 900] Past 100 steps: Average Loss 0.480 | Accuracy: 83%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.487 | Accuracy: 89%\n",
      "\n",
      "--- Testing the CNN ---\n",
      "Test Loss: 0.5633887809970116\n",
      "Test Accuracy: 0.817\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# We only use the first 1k examples of each set in the interest of time.\n",
    "# Feel free to change this if you want.\n",
    "train_images = train_images[:1000]\n",
    "train_labels = train_labels[:1000]\n",
    "test_images = test_images[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "conv = Conv3x3(8)                  # 28x28x1 -> 26x26x8\n",
    "pool = MaxPool2()                  # 26x26x8 -> 13x13x8\n",
    "softmax = Softmax(13 * 13 * 8, 10) # 13x13x8 -> 10\n",
    "\n",
    "def forward(image, label):\n",
    "  '''\n",
    "  Completes a forward pass of the CNN and calculates the accuracy and\n",
    "  cross-entropy loss.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  '''\n",
    "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "  # to work with. This is standard practice.\n",
    "  out = conv.forward((image / 255) - 0.5)\n",
    "  out = pool.forward(out)\n",
    "  out = softmax.forward(out)\n",
    "\n",
    "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
    "  loss = -np.log(out[label])\n",
    "  acc = 1 if np.argmax(out) == label else 0\n",
    "\n",
    "  return out, loss, acc\n",
    "\n",
    "def train(im, label, lr=.005):\n",
    "  '''\n",
    "  Completes a full training step on the given image and label.\n",
    "  Returns the cross-entropy loss and accuracy.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  - lr is the learning rate\n",
    "  '''\n",
    "  # Forward\n",
    "  out, loss, acc = forward(im, label)\n",
    "\n",
    "  # Calculate initial gradient\n",
    "  gradient = np.zeros(10)\n",
    "  gradient[label] = -1 / out[label]\n",
    "\n",
    "  # Backprop\n",
    "  gradient = softmax.backprop(gradient, lr)\n",
    "  gradient = pool.backprop(gradient)\n",
    "  gradient = conv.backprop(gradient, lr)\n",
    "\n",
    "  return loss, acc\n",
    "\n",
    "print('MNIST CNN initialized!')\n",
    "\n",
    "# Train the CNN for 3 epochs\n",
    "for epoch in range(3):\n",
    "  print('--- Epoch %d ---' % (epoch + 1))\n",
    "\n",
    "  # Shuffle the training data\n",
    "  permutation = np.random.permutation(len(train_images))\n",
    "  train_images = train_images[permutation]\n",
    "  train_labels = train_labels[permutation]\n",
    "\n",
    "  # Train!\n",
    "  loss = 0\n",
    "  num_correct = 0\n",
    "  for i, (im, label) in enumerate(zip(train_images, train_labels)):\n",
    "    if i % 100 == 99:\n",
    "      print(\n",
    "        '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "        (i + 1, loss / 100, num_correct)\n",
    "      )\n",
    "      loss = 0\n",
    "      num_correct = 0\n",
    "\n",
    "    l, acc = train(im, label)\n",
    "    loss += l\n",
    "    num_correct += acc\n",
    "\n",
    "# Test the CNN\n",
    "print('\\n--- Testing the CNN ---')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "for im, label in zip(test_images, test_labels):\n",
    "  _, l, acc = forward(im, label)\n",
    "  loss += l\n",
    "  num_correct += acc\n",
    "\n",
    "num_tests = len(test_images)\n",
    "print('Test Loss:', loss / num_tests)\n",
    "print('Test Accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
